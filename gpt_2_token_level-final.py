# -*- coding: utf-8 -*-
"""GPT_2_token_level.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DjKawqWr6O6UlWgDCVHoFFU8Uzj-2_h0
"""

import re
from sklearn.utils import shuffle
import numpy as np
with open('conll14st-preprocessed_copy.m2', encoding='utf8') as f:
    eval_data_letter=[]
    eval_data_token=[]  
    label=[]
    pre_line='B'
    sentence_character='S '
    anomaly_character='A ' 
    running=0
    label_array=[]
    for curr_line in f:
        m = re.search(r"http",curr_line)
        n = re.search(r"www",curr_line)
        r = re.search(r"References\n",curr_line)
        if m != None: 
            running=1
            pre_line=curr_line
        elif curr_line.isupper():
            pre_line=curr_line 
        elif n != None: 
            running=1
            pre_line=curr_line  
        elif '( ' in curr_line: 
            running=1
            pre_line=curr_line   
        elif '[ ' in curr_line: 
            running=1
            pre_line=curr_line      
        elif len(curr_line)<30:
            pre_line=curr_line  
        elif curr_line[0]==sentence_character[0]:
            label.append(label_array) 
            label_array=[]
            running=0
            curr_line=curr_line.replace('\n', "")
            curr_line=curr_line.replace(sentence_character, "")
            eval_data_letter.append(curr_line)
            curr_line=tokenizer.encode(curr_line)
            eval_data_token.append(curr_line)
            label_array=[0]*(len(curr_line))
            pre_line=curr_line
        elif curr_line[0]==anomaly_character[0]: 
            if running==0:
              numbers=curr_line[2:10]
              first_number=numbers[:numbers.index(" ")]
              second_number=numbers[numbers.index(" "):numbers.index("|")]
              second_number=second_number[1:3]
              if int(first_number) == int(second_number):
                label_array[int(first_number)-1]=1
              else:  
                label_array[int(first_number):int(second_number)]=[1]*(int(second_number)-int(first_number))               
              pre_line=curr_line
            else:
              continue  
        elif r != None: 
            running=1    
        elif running==1:
            if len(curr_line)>30:
                running=0
                curr_line=curr_line.replace('\n', "")
                curr_line=curr_line.replace(sentence_character, "")
                eval_data_letter.append(curr_line)
                curr_line=tokenizer.encode(curr_line)
                print(curr_line)
                eval_data_token.append(curr_line)
                label_array=[0]*len(curr_line)
                label.append(label_array)
                label_array=[]
                pre_line=curr_line           
            else:
                continue                   
        else : 
            pre_line=curr_line    
del(label[0])       
del(eval_data_token[-1])  
del(eval_data_letter[-1])  
#eval_data, label = shuffle(eval_data, label)
eval_data_short=eval_data_letter[0:10000]
label_short=label[0:10000]

import re
from sklearn.utils import shuffle
import numpy as np
with open('fce.test.gold.bea19.m2', encoding='utf8') as f:
    fce_letter=[]
    fce_token=[]  
    fce_label=[]
    pre_line='B'
    sentence_character='S '
    anomaly_character='A ' 
    running=0
    label_array=[]
    #regex=r"(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'\".,<>?«»“”‘’]))"
    for curr_line in f:
        m = re.search(r"http",curr_line)
        n = re.search(r"www",curr_line)
        r = re.search(r"References\n",curr_line)
        if m != None: 
            running=1
            pre_line=curr_line
        elif n != None: 
            running=1
            pre_line=curr_line  
        elif '( ' in curr_line: 
            running=1
            pre_line=curr_line   
        elif '[ ' in curr_line: 
            running=1
            pre_line=curr_line      
        elif len(curr_line)<30:
            pre_line=curr_line  
        elif curr_line[0]==sentence_character[0]:
            fce_label.append(label_array) 
            label_array=[]
            running=0
            curr_line=curr_line.replace('\n', "")
            curr_line=curr_line.replace(sentence_character, "")
            fce_letter.append(curr_line)
            curr_line=tokenizer.encode(curr_line)
            fce_token.append(curr_line)
            label_array=[0]*(len(curr_line))
            pre_line=curr_line
        elif curr_line[0]==anomaly_character[0]:        
            if running==0:
              numbers=curr_line[2:10]
              first_number=numbers[:numbers.index(" ")]
              if int(first_number)==-1:
                  pre_line=curr_line
              else:    
                  second_number=numbers[numbers.index(" "):numbers.index("|")]
                  second_number=second_number[1:3]
                  if int(first_number) == int(second_number):
                      label_array[int(first_number)-1]=1
                  else:  
                      label_array[int(first_number):int(second_number)]=[1]*(int(second_number)-int(first_number))               
                  pre_line=curr_line
            else:
                continue  
        elif r != None: 
            running=1    
        elif running==1:
            if len(curr_line)>30:
                running=0
                curr_line=curr_line.replace('\n', "")
                curr_line=curr_line.replace(sentence_character, "")
                fce_letter.append(curr_line)
                curr_line=tokenizer.encode(curr_line)
                print(curr_line)
                fce_token.append(curr_line)
                label_array=[0]*len(curr_line)
                label.append(label_array)
                label_array=[]
                pre_line=curr_line           
            else:
                continue                   
        else : 
            pre_line=curr_line 
        #print(line.strip())     
del(fce_label[0])       
del(fce_token[-1])  
del(fce_letter[-1])

import re
from sklearn.utils import shuffle
import numpy as np
with open('ABCN.dev.gold.bea19.m2', encoding='utf8') as f:
    bea19_letter=[]
    bea19_token=[]  
    bea19_label=[]
    pre_line='B'
    sentence_character='S '
    anomaly_character='A ' 
    running=0
    label_array=[]
    #regex=r"(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'\".,<>?«»“”‘’]))"
    for curr_line in f:
        m = re.search(r"http",curr_line)
        n = re.search(r"www",curr_line)
        r = re.search(r"References\n",curr_line)
        if m != None: 
            running=1
        elif n != None: 
            running=1
            pre_line=curr_line  
        elif '( ' in curr_line: 
            running=1
            pre_line=curr_line   
        elif '[ ' in curr_line: 
            running=1
            pre_line=curr_line      
        elif len(curr_line)<30:
            pre_line=curr_line  
        elif curr_line[0]==sentence_character[0]:
            bea19_label.append(label_array) 
            label_array=[]
            running=0
            curr_line=curr_line.replace('\n', "")
            curr_line=curr_line.replace(sentence_character, "")
            bea19_letter.append(curr_line)
            curr_line=tokenizer.encode(curr_line)
            bea19_token.append(curr_line)
            label_array=[0]*(len(curr_line))
            pre_line=curr_line
        elif curr_line[0]==anomaly_character[0]:        
            if running==0:
              numbers=curr_line[2:10]
              first_number=numbers[:numbers.index(" ")]
              if int(first_number)==-1:
                  pre_line=curr_line
              else:    
                  second_number=numbers[numbers.index(" "):numbers.index("|")]
                  second_number=second_number[1:3]
                  if int(first_number) == int(second_number):
                      label_array[int(first_number)-1]=1
                  else:  
                      label_array[int(first_number):int(second_number)]=[1]*(int(second_number)-int(first_number))               
                  pre_line=curr_line
            else:
                continue  
        elif r != None: 
            running=1    
        elif running==1:
            if len(curr_line)>30:
                running=0
                curr_line=curr_line.replace('\n', "")
                curr_line=curr_line.replace(sentence_character, "")
                bea19_letter.append(curr_line)
                curr_line=tokenizer.encode(curr_line)
                print(curr_line)
                bea19_token.append(curr_line)
                label_array=[0]*len(curr_line)
                label.append(label_array)
                label_array=[]
                pre_line=curr_line           
            else:
                continue                   
        else : 
            pre_line=curr_line     
del(bea19_label[0])       
del(bea19_token[-1])  
del(bea19_letter[-1])

def data_scaling(y_pred):
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    y_pred=np.array(y_pred)
    y_pred=y_pred.reshape(-1, 1)
    scaler.fit(y_pred)
    y_pred=scaler.transform(y_pred)
    return y_pred

def data_cleaner (eval_data):
    clean_data=[]
    for line in eval_data:
        line=line.replace('\n', "")      
        clean_data.append(line)
    return clean_data       

def get_threshold (y_true, y_pred):
    from sklearn.metrics import roc_curve, auc
    from matplotlib import pyplot
    y_pred = list(np.concatenate(y_pred).flat)
    y_true = list(np.concatenate(y_true). flat)
    fpr, tpr, thresholds =roc_curve(y_true, y_pred)
    gmeans = np.sqrt(tpr * (1-fpr))
    ix = np.argmax(gmeans)
    print('Best Threshold=', thresholds[ix])
    pyplot.plot(fpr, tpr, marker='.')
    # axis labels
    pyplot.xlabel('False Positive Rate')
    pyplot.ylabel('True Positive Rate')
    # show the legend
    pyplot.legend()
    # show the plot
    pyplot.show()
    return thresholds[ix+1]
def searching_word(search_word, clean_data):
    matching=[]
    matching_index=[]
    search_word=search_word+' '
    matching = [sentence for i, sentence in enumerate(clean_data) if search_word in sentence]
    matching_index = [i for i, sentence in enumerate(clean_data) if search_word in sentence]
    #for i, sentence in enumerate(clean_data):
    #        if search_word in sentence:
    #            matching.append(sentence)  
    #            matching_index.append(i)  
    return matching, matching_index
def get_result (answer, threshold):
    result = [1 if sentence<threshold else 0 for sentence in answer]
    return result
def count_occurance (data):
    newlist = [item for items in data for item in items]
    return newlist

import tensorflow as tf
import numpy as np
import torch
from torch import nn
from scipy.stats import gmean

def detect_anamoly_baseline_token(dataset, best_probability): 
    result=[] 
    word_probability=[] 
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    cosine_loss=tf.keras.losses.CosineSimilarity(axis=-1, name='cosine_similarity')
    data_converted=[]    
    sentence_probability=[]
    m = nn.Softmax(dim=1)
    weights = model.transformer.wte.weight
    weights = weights.detach().numpy()
    for index, eval_sentence in enumerate(dataset): 
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        outputs = model(**inputs, labels=inputs["input_ids"]) 
        entire_sentence_word_p=[0]
        for i in range(1, len(tokens_tensor)): 
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()  
            if word_probability < best_probability: 
                word_probability=1
            else: 
                word_probability=0      
            entire_sentence_word_p.append(word_probability) 
    return sentence_probability

import tensorflow as tf
import tensorflow as tf
import numpy as np
import torch
from torch import nn
from scipy.stats import gmean

def detect_anamoly_frequency_token(dataset, corpus, best_probability): 
    result=[] 
    word_probability=[] 
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    cosine_loss=tf.keras.losses.CosineSimilarity(axis=-1, name='cosine_similarity')
    data_converted=[]    
    sentence_probability=[]
    m = nn.Softmax(dim=1)
    brown_data_tokenized=tokenizer(corpus)
    temp_count_list=count_occurance(brown_data_tokenized['input_ids'])
    unique, counts = np.unique(temp_count_list, return_counts=True)
    temp_count_list=np.array(temp_count_list)
    for index, eval_sentence in enumerate(dataset): 
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[0]
        for i in range(1, len(tokens_tensor)): 
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()
            softmax_output=softmax_output.detach().numpy()
            word_count = temp_count_list[temp_count_list==curr_word].shape[0]
            all_count=np.sum(counts)
            frequency_p=word_count/all_count 
            if frequency_p==0:
                word_p=word_probability
            else: 
                word_p=word_probability/frequency_p
            if word_p < best_probability:
                word_p=1
            else:
                word_p=0
            entire_sentence_word_p.append(word_p)
        sentence_probability.append(entire_sentence_word_p)
    return sentence_probability

import tensorflow as tf
import tensorflow as tf
import numpy as np
import torch
from torch import nn
from scipy.stats import gmean

def detect_anamoly_frequency_token_threshold(dataset, corpus, y_test, best_probability): 
    result=[] 
    word_probability=[] 
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    cosine_loss=tf.keras.losses.CosineSimilarity(axis=-1, name='cosine_similarity')
    data_converted=[]    
    sentence_probability=[]
    m = nn.Softmax(dim=1)
    brown_data_tokenized=tokenizer(corpus)
    temp_count_list=count_occurance(brown_data_tokenized['input_ids'])
    unique, counts = np.unique(temp_count_list, return_counts=True)
    temp_count_list=np.array(temp_count_list)
    for index, eval_sentence in enumerate(dataset): 
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[100]
        for i in range(1, len(tokens_tensor)): 
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()
            softmax_output=softmax_output.detach().numpy()
            word_count = temp_count_list[temp_count_list==curr_word].shape[0]
            all_count=np.sum(counts)
            frequency_p=word_count/all_count 
            if frequency_p==0:
                word_p=word_probability
            else: 
                word_p=word_probability/frequency_p
            entire_sentence_word_p.append(word_p)
        sentence_probability.append(entire_sentence_word_p)
    threshold=get_threshold(y_test, sentence_probability)
    sentence_probability=list(np.concatenate(sentence_probability).flat)
    result=[1 if word<threshold else 0 for word in sentence_probability]  
    return result

import tensorflow as tf
import tensorflow as tf
import numpy as np
import torch
from torch import nn
from scipy.stats import gmean
from sklearn.metrics import f1_score

def detect_anamoly_cosine_token(dataset, best_probability, cosine_threshold, k): 
    result=[] 
    word_probability=[] 
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    cosine_loss=tf.keras.losses.CosineSimilarity(axis=-1, name='cosine_similarity')
    data_converted=[]    
    sentence_probability=[]
    weights = model.transformer.wte.weight
    weights = weights.detach().numpy()
    m = nn.Softmax(dim=1)
    for index, eval_sentence in enumerate(dataset): 
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False)
        outputs = model(**inputs, labels=inputs["input_ids"])
        entire_sentence_word_p=[0]
        for i in range(1, len(tokens_tensor)): 
            pre_word=tokens_tensor[i-1] 
            softmax_output = m(outputs[1][:,i-1,:])
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()
            softmax_output=softmax_output.detach().numpy()
            best_word2=np.argmax(softmax_output)
            if word_probability < best_probability: 
                        append_number=1
                        idx = (outputs[1][:,i-1,:]).argsort()[0][k:]
                        for best_word in idx:  
                            loss=cosine_loss(weights[curr_word], weights[best_word]).numpy() 
                            if loss<cosine_threshold:
                                append_number=0
                                break
                            else:                            
                                continue
                        entire_sentence_word_p.append(append_number)                        
            elif word_probability==None:
                entire_sentence_word_p.append(1)
            else:
                entire_sentence_word_p.append(0)
        sentence_probability.append(entire_sentence_word_p)
    return sentence_probability

import tensorflow as tf
import tensorflow as tf
import numpy as np
import torch
from torch import nn
from scipy.stats import gmean

def detect_anamoly_position_token(dataset, best_probability): 
    result=[] 
    word_probability=[] 
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    cosine_loss=tf.keras.losses.CosineSimilarity(axis=-1, name='cosine_similarity')
    data_converted=[]    
    sentence_probability=[]
    m = nn.Softmax(dim=1)
    weights = model.transformer.wte.weight
    weights = weights.detach().numpy()
    for eval_sentence in dataset: 
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[0]
        for i in range(1, len(tokens_tensor)): 
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()  
            entire_position_probability=m(outputs[1][:, :,curr_word])
            position_probability=entire_position_probability[0][i-1]
            position_probability=position_probability.detach().numpy()
            word_position_probability=position_probability/1
            word_probability=np.log(word_probability)
            if word_position_probability != 0.0:
              word_position_probability=np.log(word_position_probability)
            word_probability_conditioned_position=word_probability-word_position_probability
            if word_probability_conditioned_position < best_probability: 
                word_probability_conditioned_position=1
            else:
                word_probability_conditioned_position=0 
            entire_sentence_word_p.append(word_probability_conditioned_position)
        sentence_probability.append(entire_sentence_word_p)          
    return sentence_probability

import tensorflow as tf
import tensorflow as tf
import numpy as np
import torch
from torch import nn
from scipy.stats import gmean

def detect_anamoly_position_token_threshold(dataset, best_probability, y_test): 
    result=[] 
    word_probability=[] 
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    cosine_loss=tf.keras.losses.CosineSimilarity(axis=-1, name='cosine_similarity')
    data_converted=[]    
    sentence_probability=[]
    m = nn.Softmax(dim=1)
    weights = model.transformer.wte.weight
    weights = weights.detach().numpy()
    for index, eval_sentence in enumerate(dataset): 
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[0]
        for i in range(1, len(tokens_tensor)): 
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()  
            entire_position_probability=m(outputs[1][:, :,curr_word])
            position_probability=entire_position_probability[0][i-1]
            position_probability=position_probability.detach().numpy()
            word_position_probability=position_probability/1
            word_probability=np.log(word_probability)
            if word_position_probability != 0.0:
              word_position_probability=np.log(word_position_probability)
            word_probability_conditioned_position=word_probability-word_position_probability
            entire_sentence_word_p.append(word_probability_conditioned_position)
        sentence_probability.append(entire_sentence_word_p)   
    threshold=get_threshold(y_test, sentence_probability)
    sentence_probability=list(np.concatenate(sentence_probability).flat)
    result=[1 if word>threshold else 0 for word in sentence_probability]         
    return result