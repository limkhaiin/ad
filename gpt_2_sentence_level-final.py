# -*- coding: utf-8 -*-
"""GPT_2_sentence_level-final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j2RprswPnh1BVXGH917hr-Fskx_ao2VP
"""

!pip install transformers
from transformers import GPT2Tokenizer, GPT2LMHeadModel

import os
# the base Google Drive directory
root_dir = "/content/"

# choose where you want your project files to be saved
project_folder = "drive/MyDrive/Colab Notebooks/GPT-2/"
os.chdir(root_dir + project_folder)

import re
from sklearn.utils import shuffle
with open('conll14st-preprocessed_copy.m2', encoding='utf8') as f:
    eval_data=[]  
    label=[]
    pre_line='B'
    sentence_character='S '
    anomaly_character='A ' 
    running=0
    for curr_line in f:
        m = re.search(r"http",pre_line)
        n = re.search(r"www",pre_line)
        r = re.search(r"References\n",pre_line)
        if m != None: 
            pre_line=curr_line
        elif pre_line.isupper():
            pre_line=curr_line 
        elif n != None: 
            pre_line=curr_line  
        elif '( ' in pre_line: 
            pre_line=curr_line   
        elif '[ ' in pre_line: 
            pre_line=curr_line      
        elif len(pre_line)<30:
            pre_line=curr_line    
        elif curr_line[0]==anomaly_character[0] and pre_line[0]==sentence_character[0]: 
            pre_line=pre_line.replace(sentence_character, "")
            eval_data.append(pre_line)
            label.append(1)
            pre_line=curr_line
        elif r != None: 
            running=1    
        elif running==1:
            if len(pre_line)>30:
                running=0
                pre_line=pre_line.replace(sentence_character, "")
                eval_data.append(pre_line)
                label.append(0)
                pre_line=curr_line           
            else:
                continue                   
        elif pre_line[0]==sentence_character[0]: 
            pre_line=pre_line.replace(sentence_character, "")
            eval_data.append(pre_line)
            label.append(0)
            pre_line=curr_line
        else : 
            pre_line=curr_line 
eval_data=data_cleaner(eval_data)        
#eval_data, label = shuffle(eval_data, label)
eval_data_short=eval_data[0:10000]
label_short=label[0:10000]

import re
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import numpy as np
with open('ABCN.dev.gold.bea19.m2', encoding='utf8') as f:
    bea19_data=[] 
    bea19_label=[]
    pre_line='B'
    sentence_character='S '
    anomaly_character='A ' 
    running=0
    label_array=0
    for curr_line in f:
        m = re.search(r"http",curr_line)
        n = re.search(r"www",curr_line)
        r = re.search(r"References\n",curr_line)
        if m != None:         
            pre_line=curr_line
        elif n != None:         
            pre_line=curr_line  
        elif '( ' in curr_line:          
            pre_line=curr_line   
        elif '[ ' in curr_line:         
            pre_line=curr_line      
        elif len(curr_line)<20:
            pre_line=curr_line 
        elif curr_line[0]==sentence_character[0]: 
            bea19_label.append(label_array) 
            label_array=int()
            curr_line=curr_line.replace('\n', "")
            curr_line=curr_line.replace(sentence_character, "")
            bea19_data.append(curr_line)
            label_array=0
            pre_line=curr_line    
        elif curr_line[0]==anomaly_character[0]:
            numbers=curr_line[2:10]
            first_number=numbers[:numbers.index(" ")]
            if int(first_number)==-1:
                  pre_line=curr_line
            else:    
                  label_array=1           
                  pre_line=curr_line                                 
        else : 
            pre_line=curr_line 
        #print(line.strip())  

del(bea19_label[0])       
del(bea19_data[-1])  
bea19_data, bea19_label = shuffle(bea19_data[100:], bea19_label[100:])   
X_train, X_test, y_train, y_test = train_test_split(bea19_data, bea19_label, test_size=0.3)

from sklearn.utils import shuffle
with open('entries.train', encoding='utf8') as f:
    eval_data_lang8=[] 
    eval_label_lang8=[] 
    for curr_line in f:
        if len(curr_line)<5:
            continue
        else:    
            li=list(curr_line.split("\t"))
            eval_data_lang8.append(li[4])
            eval_label_lang8.append(int(li[0]))
            
clean_eval_label_lang8=[]
for label in eval_label_lang8:
    if label>1:
        label=1
        clean_eval_label_lang8.append(label)
    else:    
        clean_eval_label_lang8.append(label)
clean_eval_data_lang8=data_cleaner(eval_data_lang8)
#clean_eval_data_lang8, clean_eval_label_lang8 = shuffle(clean_eval_data_lang8, clean_eval_label_lang8)
clean_eval_data_lang8_short=clean_eval_data_lang8[0:10000]
clean_eval_label_lang8_short=clean_eval_label_lang8[0:10000]

def data_scaling(y_pred):
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    y_pred=np.array(y_pred)
    y_pred=y_pred.reshape(-1, 1)
    scaler.fit(y_pred)
    y_pred=scaler.transform(y_pred)
    return y_pred

def data_cleaner (eval_data):
    clean_data=[]
    for line in eval_data:
        line=line.replace('\n', "")
        #re.sub(r'\([^)]*\)', '', line)       
        clean_data.append(line)
    return clean_data       

def count_occurance (data):
    newlist = [item for items in data for item in items]
    return newlist

def get_threshold (y_true, y_pred):
    from sklearn.metrics import roc_curve, auc
    from matplotlib import pyplot
    fpr, tpr, thresholds =roc_curve(y_true, y_pred)
    gmeans = np.sqrt(tpr * (1-fpr))
    ix = np.argmax(gmeans)
    print('Best Threshold=', thresholds[ix])
    pyplot.plot(fpr, tpr, marker='.')
    # axis labels
    pyplot.xlabel('False Positive Rate')
    pyplot.ylabel('True Positive Rate')
    # show the legend
    pyplot.legend()
    # show the plot
    pyplot.show()
    return thresholds[ix+1]
def searching_word(search_word, clean_data):
    matching=[]
    matching_index=[]
    search_word=search_word+' '
    matching = [sentence for i, sentence in enumerate(clean_data) if search_word in sentence]
    matching_index = [i for i, sentence in enumerate(clean_data) if search_word in sentence] 
    return matching, matching_index

def get_result (answer, threshold):
    result = [1 if sentence<threshold else 0 for sentence in answer]
    return result

import tensorflow as tf
import tensorflow as tf
import numpy as np
import torch
from torch import nn
from scipy.stats import gmean

def detect_anamoly_sentence_frequency(dataset, corpus, best_probability): 
    result=[] 
    word_probability=[] 
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    cosine_loss=tf.keras.losses.CosineSimilarity(axis=-1, name='cosine_similarity')
    data_converted=[]    
    sentence_probability=[]
    m = nn.Softmax(dim=1)
    brown_data_tokenized=tokenizer(corpus)
    temp_count_list=count_occurance(brown_data_tokenized['input_ids'])
    unique, counts = np.unique(temp_count_list, return_counts=True)
    temp_count_list=np.array(temp_count_list)
    for index, eval_sentence in enumerate(dataset): 
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[]
        for i in range(1, len(tokens_tensor)): 
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()
            softmax_output=softmax_output.detach().numpy()
            word_count = temp_count_list[temp_count_list==curr_word].shape[0]
            all_count=np.sum(counts)
            frequency_p=word_count/all_count 
            if frequency_p==0:
                word_p=word_probability
            else: 
                word_p=word_probability/frequency_p
            if word_p<best_probability:
                word_p= -(best_probability)*(len(tokens_tensor))
            entire_sentence_word_p.append(word_p)       
        if len(entire_sentence_word_p)==0 :
            sentence_probability.append(0.0)
        else:
            entire_sentence_word_probability_mean=np.mean(entire_sentence_word_p) 
            sentence_probability.append(entire_sentence_word_probability_mean)          
    for sentence in sentence_probability:
        if sentence<0:
            result.append(1)
        else:
            result.append(0)     
    return result

import tensorflow as tf
import tensorflow as tf
import numpy as np
import torch
from torch import nn
from scipy.stats import gmean

def detect_anamoly_sentence_frequency_threshold(dataset, corpus, y_test): 
    result=[] 
    word_probability=[] 
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    cosine_loss=tf.keras.losses.CosineSimilarity(axis=-1, name='cosine_similarity')
    data_converted=[]    
    sentence_probability=[]
    m = nn.Softmax(dim=1)
    brown_data_tokenized=tokenizer(corpus)
    temp_count_list=count_occurance(brown_data_tokenized['input_ids'])
    unique, counts = np.unique(temp_count_list, return_counts=True)
    temp_count_list=np.array(temp_count_list)
    for index, eval_sentence in enumerate(dataset): 
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[]
        for i in range(1, len(tokens_tensor)): 
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()
            softmax_output=softmax_output.detach().numpy()
            word_count = temp_count_list[temp_count_list==curr_word].shape[0]
            all_count=np.sum(counts)
            frequency_p=word_count/all_count 
            if frequency_p==0:
                word_p=word_probability
            else: 
                word_p=word_probability/frequency_p
            entire_sentence_word_p.append(word_p)
        entire_sentence_word_probability_mean=np.mean(entire_sentence_word_p) 
        if np.isnan(entire_sentence_word_probability_mean).any()==True :
            sentence_probability.append(0.0)
        else:
            sentence_probability.append(entire_sentence_word_probability_mean)
    threshold=get_threshold(y_test, sentence_probability)
    for sentence in sentence_probability:
        if sentence<threshold:
            result.append(1)
        else:
            result.append(0)     
    return result

import tensorflow as tf
import tensorflow as tf
import numpy as np
import torch
from torch import nn
from scipy.stats import gmean

def detect_anamoly_sentence_baseline(dataset, best_probability): 
    result=[] 
    word_probability=[] 
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    cosine_loss=tf.keras.losses.CosineSimilarity(axis=-1, name='cosine_similarity')
    data_converted=[]    
    sentence_probability=[]
    m = nn.Softmax(dim=1)
    weights = model.transformer.wte.weight
    weights = weights.detach().numpy()
    for index, eval_sentence in enumerate(dataset): 
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[]
        for i in range(1, len(tokens_tensor)-1): 
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()  
            if word_probability < best_probability: 
               word_probability=-(len(tokens_tensor))   
            entire_sentence_word_p.append(word_probability)   
        entire_sentence_word_probability_mean=np.mean(entire_sentence_word_p)  
        if np.isnan(entire_sentence_word_probability_mean).any()==True :
            sentence_probability.append(-10.0)
        else:
            sentence_probability.append(entire_sentence_word_probability_mean)
    result=get_result(sentence_probability, 0)   
    return result

import tensorflow as tf
import tensorflow as tf
import numpy as np
import torch
from torch import nn
from scipy.stats import gmean
from sklearn.metrics import f1_score

def detect_anamoly_sentence_cosine(dataset, best_probability, cosine_threshold, k): 
    result=[] 
    word_probability=[] 
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    cosine_loss=tf.keras.losses.CosineSimilarity(axis=-1, name='cosine_similarity')
    data_converted=[]    
    sentence_probability=[]
    weights = model.transformer.wte.weight
    weights = weights.detach().numpy()
    m = nn.Softmax(dim=1)
    for index, eval_sentence in enumerate(dataset): 
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False)
        outputs = model(**inputs, labels=inputs["input_ids"])
        entire_sentence_word_p=[]
        for i in range(1, len(tokens_tensor)-1): 
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()
            softmax_output=softmax_output.detach().numpy()
            best_word2=np.argmax(softmax_output)
            if word_probability < best_probability: 
                        append_number=-23
                        idx = (outputs[1][:,i-1,:]).argsort()[0][k:]
                        for best_word in idx:  
                            loss=cosine_loss(weights[curr_word], weights[best_word]).numpy() 
                            if loss<cosine_threshold:
                                append_number=word_probability
                                break
                            else:                            
                                continue
                        entire_sentence_word_p.append(append_number)                        
            elif word_probability==None:
                entire_sentence_word_p.append(-23)
            else:
                entire_sentence_word_p.append(word_probability)
        entire_sentence_word_p=np.array(entire_sentence_word_p)   
        entire_sentence_word_probability_mean=np.mean(entire_sentence_word_p)
        if (np.isnan(entire_sentence_word_probability_mean)).any()==True:
            entire_sentence_word_probability_mean=-10
        sentence_probability.append(entire_sentence_word_probability_mean)
    result=get_result(sentence_probability, 0)  
    return result

import tensorflow as tf
import tensorflow as tf
import numpy as np
import torch
from torch import nn
from scipy.stats import gmean

def detect_anamoly_sentence_position(dataset, best_probability): 
    result=[] 
    word_probability=[] 
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    cosine_loss=tf.keras.losses.CosineSimilarity(axis=-1, name='cosine_similarity')
    data_converted=[]    
    sentence_probability=[]
    m = nn.Softmax(dim=1)
    weights = model.transformer.wte.weight
    weights = weights.detach().numpy()
    for index, eval_sentence in enumerate(dataset): 
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[]
        for i in range(1, len(tokens_tensor)-1): 
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()  
            entire_position_probability=m(outputs[1][:, :,curr_word])
            position_probability=entire_position_probability[0][i-1]
            position_probability=position_probability.detach().numpy()
            word_position_probability=position_probability/1
            word_probability=np.log(word_probability)
            if word_position_probability != 0.0:
              word_position_probability=np.log(word_position_probability)
            word_probability_conditioned_position=word_probability-word_position_probability
            if word_probability_conditioned_position > best_probability:   
              word_probability_conditioned_position=best_probability*len(tokens_tensor)
            entire_sentence_word_p.append(word_probability_conditioned_position)        
        if len(entire_sentence_word_p)==0 :
            sentence_probability.append(0.0)
        else:
            entire_sentence_word_probability_mean=np.mean(entire_sentence_word_p) 
            sentence_probability.append(entire_sentence_word_probability_mean)           
    for sentence in sentence_probability:
        if sentence>=best_probability:
            result.append(1)
        else:
            result.append(0)        
    return result

import tensorflow as tf
import tensorflow as tf
import numpy as np
import torch
from torch import nn
from scipy.stats import gmean

def detect_anamoly_sentence_position_threshold(dataset, y_test): 
    result=[] 
    word_probability=[] 
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    cosine_loss=tf.keras.losses.CosineSimilarity(axis=-1, name='cosine_similarity')
    data_converted=[]    
    sentence_probability=[]
    m = nn.Softmax(dim=1)
    weights = model.transformer.wte.weight
    weights = weights.detach().numpy()
    for eval_sentence in dataset: 
        #print("eval_sentence is", eval_sentence)
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
#        tokens_tensor=inputs['input_ids'][0]
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[]
        for i in range(1, len(tokens_tensor)-1): 
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()  
            entire_position_probability=m(outputs[1][:, :,curr_word])
            position_probability=entire_position_probability[0][i-1]
            position_probability=position_probability.detach().numpy()
            word_position_probability=position_probability/1
            word_probability=np.log(word_probability)
            if word_position_probability != 0.0:
              word_position_probability=np.log(word_position_probability)
            word_probability_conditioned_position=word_probability-word_position_probability
            entire_sentence_word_p.append(word_probability_conditioned_position)
        if len(entire_sentence_word_p)==0 :
            sentence_probability.append(0.0)
        else:
            entire_sentence_word_probability_mean=np.amin(entire_sentence_word_p) 
            sentence_probability.append(entire_sentence_word_probability_mean)    
    threshold=get_threshold(y_test, sentence_probability)
    for sentence in sentence_probability:
        if sentence<threshold:
            result.append(1)
        else:
            result.append(0)        
    return result