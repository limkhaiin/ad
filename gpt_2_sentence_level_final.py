# -*- coding: utf-8 -*-
"""GPT_2_sentence_level-final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j2RprswPnh1BVXGH917hr-Fskx_ao2VP
"""

#import pacakges 
!pip install transformers
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import os
import re
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import numpy as np
import tensorflow as tf
import torch
from torch import nn
from scipy.stats import gmean
from sklearn.metrics import f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve, auc
from matplotlib import pyplot

# the base Google Drive directory
root_dir = "/content/"

# choose where you want your project files to be saved
project_folder = "drive/MyDrive/Colab Notebooks/GPT-2/"
os.chdir(root_dir + project_folder)

#load coll-14 dataset and preprocessing
with open('conll14st-preprocessed_copy.m2', encoding='utf8') as f:
    eval_data=[]  
    label=[]
    pre_line='B'
    sentence_character='S '
    anomaly_character='A ' 
    running=0
    for curr_line in f:
        m = re.search(r"http",pre_line)
        n = re.search(r"www",pre_line)
        r = re.search(r"References\n",pre_line)
        if m != None: 
            pre_line=curr_line
        elif pre_line.isupper():
            pre_line=curr_line 
        elif n != None: 
            pre_line=curr_line  
        elif '( ' in pre_line: 
            pre_line=curr_line   
        elif '[ ' in pre_line: 
            pre_line=curr_line      
        elif len(pre_line)<30:
            pre_line=curr_line    
        elif curr_line[0]==anomaly_character[0] and pre_line[0]==sentence_character[0]: 
            pre_line=pre_line.replace(sentence_character, "")
            eval_data.append(pre_line)
            label.append(1)
            pre_line=curr_line
        elif r != None: 
            running=1    
        elif running==1:
            if len(pre_line)>30:
                running=0
                pre_line=pre_line.replace(sentence_character, "")
                eval_data.append(pre_line)
                label.append(0)
                pre_line=curr_line           
            else:
                continue                   
        elif pre_line[0]==sentence_character[0]: 
            pre_line=pre_line.replace(sentence_character, "")
            eval_data.append(pre_line)
            label.append(0)
            pre_line=curr_line
        else : 
            pre_line=curr_line 
eval_data=data_cleaner(eval_data)        
#eval_data, label = shuffle(eval_data, label)
eval_data_short=eval_data[0:10000]
label_short=label[0:10000]

#load bea-19 dataset and preprocessing
with open('ABCN.dev.gold.bea19.m2', encoding='utf8') as f:
    bea19_data=[] 
    bea19_label=[]
    pre_line='B'
    sentence_character='S '
    anomaly_character='A ' 
    running=0
    label_array=0
    for curr_line in f:
        m = re.search(r"http",curr_line)
        n = re.search(r"www",curr_line)
        r = re.search(r"References\n",curr_line)
        if m != None:         
            pre_line=curr_line
        elif n != None:         
            pre_line=curr_line  
        elif '( ' in curr_line:          
            pre_line=curr_line   
        elif '[ ' in curr_line:         
            pre_line=curr_line      
        elif len(curr_line)<20:
            pre_line=curr_line 
        elif curr_line[0]==sentence_character[0]: 
            bea19_label.append(label_array) 
            label_array=int()
            curr_line=curr_line.replace('\n', "")
            curr_line=curr_line.replace(sentence_character, "")
            bea19_data.append(curr_line)
            label_array=0
            pre_line=curr_line    
        elif curr_line[0]==anomaly_character[0]:
            numbers=curr_line[2:10]
            first_number=numbers[:numbers.index(" ")]
            if int(first_number)==-1:
                  pre_line=curr_line
            else:    
                  label_array=1           
                  pre_line=curr_line                                 
        else : 
            pre_line=curr_line 
        #print(line.strip())  

del(bea19_label[0])       
del(bea19_data[-1])  
bea19_data, bea19_label = shuffle(bea19_data[100:], bea19_label[100:])   
X_train, X_test, y_train, y_test = train_test_split(bea19_data, bea19_label, test_size=0.3)

#load lang-8 dataset and preprocessing
with open('entries.train', encoding='utf8') as f:
    eval_data_lang8=[] 
    eval_label_lang8=[] 
    for curr_line in f:
        if len(curr_line)<5:
            continue
        else:    
            li=list(curr_line.split("\t"))
            eval_data_lang8.append(li[4])
            eval_label_lang8.append(int(li[0]))
            
clean_eval_label_lang8=[]
for label in eval_label_lang8:
    if label>1:
        label=1
        clean_eval_label_lang8.append(label)
    else:    
        clean_eval_label_lang8.append(label)
clean_eval_data_lang8=data_cleaner(eval_data_lang8)
#clean_eval_data_lang8, clean_eval_label_lang8 = shuffle(clean_eval_data_lang8, clean_eval_label_lang8)
clean_eval_data_lang8_short=clean_eval_data_lang8[0:10000]
clean_eval_label_lang8_short=clean_eval_label_lang8[0:10000]

# remove escape sequence from the data
def data_cleaner (eval_data):
    clean_data=[]
    for line in eval_data:
        line=line.replace('\n', "")
        #re.sub(r'\([^)]*\)', '', line)       
        clean_data.append(line)
    return clean_data       

# function for counting occurrence in frequency method
def count_occurance (data):
    newlist = [item for items in data for item in items]
    return newlist

# Compute the optimal threshold using Yoden's index
def get_threshold (y_true, y_pred):
    fpr, tpr, thresholds =roc_curve(y_true, y_pred)
    gmeans = np.sqrt(tpr * (1-fpr))
    ix = np.argmax(gmeans)
    print('Best Threshold=', thresholds[ix])
    pyplot.plot(fpr, tpr, marker='.')
    # axis labels
    pyplot.xlabel('False Positive Rate')
    pyplot.ylabel('True Positive Rate')
    # show the legend
    pyplot.legend()
    # show the plot
    pyplot.show()
    return thresholds[ix+1]

# simply classifier to judge the grammaticality of sentences
# if sentence probability is negative, it is an anomalous sentence, otherwirse a natural sentence  
def get_result (answer, threshold):
    result = [1 if sentence<threshold else 0 for sentence in answer]
    return result

# baseline method (3.1)
def detect_anamoly_sentence_baseline(dataset, best_probability): 
    result=[] 
    word_probability=[] 
    #import GPT-2
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    #import tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    data_converted=[]    
    sentence_probability=[]
    # import softmax layer
    m = nn.Softmax(dim=1)
    # iterate through all the sentences in the evaluation set   
    for index, eval_sentence in enumerate(dataset): 
        # tokenize words in sentences to evaluate
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        # get next-word probabilities of words in a evaluation sentence
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[]
        # generate the probabilties of words in evaluation sentence using the relevant next-word probabilities of GPT-2
        for i in range(1, len(tokens_tensor)-1): 
            # get the current token and the previous token to examine
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            # get the next-word probability of the current word
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()  
            # convert the next-word probability of the current word to a negative number if the next-word probability of the current word is lower than the threshold
            if word_probability < best_probability: 
               word_probability=-(len(tokens_tensor))   
            entire_sentence_word_p.append(word_probability) 
        # get the mean of next-word probabilities in a sentence      
        entire_sentence_word_probability_mean=np.mean(entire_sentence_word_p)  
        # process the case when sentence list contains nothing 
        if np.isnan(entire_sentence_word_probability_mean).any()==True :
            sentence_probability.append(-10.0)
        else:
            sentence_probability.append(entire_sentence_word_probability_mean)
    # if sentence probability is negative, it is an anomalous sentence, otherwirse a natural sentence          
    result=get_result(sentence_probability, 0)   
    return result

# frequency-conditioned method (3.2)
def detect_anamoly_sentence_frequency_threshold(dataset, corpus, y_test): 
    result=[] 
    word_probability=[] 
    #import GPT-2
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    #import tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    data_converted=[]    
    sentence_probability=[]
    # import softmax layer
    m = nn.Softmax(dim=1)
    # import BROWN corpus
    brown_data_tokenized=tokenizer(corpus)
    # count the occurence of all the words in the corpus
    temp_count_list=count_occurance(brown_data_tokenized['input_ids'])
    unique, counts = np.unique(temp_count_list, return_counts=True)
    temp_count_list=np.array(temp_count_list)
    # iterate through all the sentences in the evaluation set
    for index, eval_sentence in enumerate(dataset): 
        # tokenize words in sentences to evaluate
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        # get next-word probabilities of words in a evaluation sentence
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[]
        # generate the probabilties of words in a evaluation sentence using the relevant next-word probabilities of GPT-2
        for i in range(1, len(tokens_tensor)): 
            # get the current token and the previous token to examine
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            # get the next-word probability of the current word
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()
            softmax_output=softmax_output.detach().numpy()
            # compute the wordcounts of the current word
            word_count = temp_count_list[temp_count_list==curr_word].shape[0]
            all_count=np.sum(counts)
            # get the frequency of the current word
            frequency_p=word_count/all_count 
            # avoid denominator to be zero and normalize the probability of the current word
            if frequency_p==0:
                word_p=word_probability
            else: 
                word_p=word_probability/frequency_p
            # append the next-word probability of the current word to a sentence list    
            entire_sentence_word_p.append(word_p)
        # get the mean of next-word probabilities in a sentence 
        entire_sentence_word_probability_mean=np.mean(entire_sentence_word_p)
        # process the case when sentence list contains nothing  
        if np.isnan(entire_sentence_word_probability_mean).any()==True :
            sentence_probability.append(0.0)
        else:
            sentence_probability.append(entire_sentence_word_probability_mean)
    # Use Yoden's Index to find an optimal threshold        
    threshold=get_threshold(y_test, sentence_probability)
    # use the optimal threshold to detect the anomalous sentence
    for sentence in sentence_probability:
        if sentence<threshold:
            result.append(1)
        else:
            result.append(0)     
    return result

# frequency-conditioned method (3.2)
def detect_anamoly_sentence_frequency(dataset, corpus, best_probability): 
    result=[] 
    word_probability=[] 
    #import GPT-2
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    #import tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    data_converted=[]    
    sentence_probability=[]
    # import softmax layer
    m = nn.Softmax(dim=1)
    # import BROWN corpus
    brown_data_tokenized=tokenizer(corpus)
    # count the occurence of all the words in the corpus
    temp_count_list=count_occurance(brown_data_tokenized['input_ids'])
    unique, counts = np.unique(temp_count_list, return_counts=True)
    temp_count_list=np.array(temp_count_list)
    # iterate through all the sentences in the evaluation set 
    for index, eval_sentence in enumerate(dataset): 
        # tokenize words in sentences to evaluate
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        # get next-word probabilities of words in a evaluation sentence
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[]
        # generate the probabilties of words in evaluation sentence using the relevant next-word probabilities of GPT-2
        for i in range(1, len(tokens_tensor)): 
            # get the current token and the previous token to examine
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            # get the next-word probability of the current word
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()
            softmax_output=softmax_output.detach().numpy()
            # compute the wordcounts of the current word
            word_count = temp_count_list[temp_count_list==curr_word].shape[0]
            all_count=np.sum(counts)
            # get the frequency of the current word
            frequency_p=word_count/all_count 
            # avoid denominator to be zero and normalize the probability of the current word
            if frequency_p==0:
                word_p=word_probability
            else: 
                word_p=word_probability/frequency_p
            # convert the next-word probability of the current word to a negative number if the next-word probability of the current word is lower than the threshold
            if word_p<best_probability:
                word_p= -(best_probability)*(len(tokens_tensor))
            # append the normalized/processed next-word probability to a sentence list   
            entire_sentence_word_p.append(word_p) 
        # process the case when sentence list contains nothing          
        if len(entire_sentence_word_p)==0 :
            sentence_probability.append(0.0)
        else:
        #get the mean of next-word probabilities in a sentence  
            entire_sentence_word_probability_mean=np.mean(entire_sentence_word_p) 
            sentence_probability.append(entire_sentence_word_probability_mean)   
    # if sentence probability is negative, it is an anomalous sentence, otherwirse a natural sentence               
    for sentence in sentence_probability:
        if sentence<0:
            result.append(1)
        else:
            result.append(0)     
    return result

# Word Embeddings Cosine Similarity method (3.3)
def detect_anamoly_sentence_cosine(dataset, best_probability, cosine_threshold, k): 
    result=[] 
    word_probability=[] 
    #import GPT-2
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    #import tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    # import cosine similairty loss function
    cosine_loss=tf.keras.losses.CosineSimilarity(axis=-1, name='cosine_similarity')
    data_converted=[]    
    sentence_probability=[]
    # import word-embeddings in GPT-2
    weights = model.transformer.wte.weight
    weights = weights.detach().numpy()
    # import softmax layer
    m = nn.Softmax(dim=1)
    # iterate through all the sentences in the evaluation set 
    for index, eval_sentence in enumerate(dataset): 
        # tokenize words in sentences to evaluate
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        # get next-word probabilities of words in a evaluation sentence
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False)
        outputs = model(**inputs, labels=inputs["input_ids"])
        entire_sentence_word_p=[]
        # generate the probabilties of words in a evaluation sentence using the relevant next-word probabilities of GPT-2
        for i in range(1, len(tokens_tensor)-1): 
            # get the current token and the previous token to examine
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            # get the next-word probability of the current word
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()
            softmax_output=softmax_output.detach().numpy()
            # if the next-word probability of the current word is lower than a threshold, compute k most probable words of the previous word 
            if word_probability < best_probability: 
                        append_number=-23
                        idx = (outputs[1][:,i-1,:]).argsort()[0][k:]
                        # iterate through k most probable words
                        for best_word in idx:  
                            # compute the cosine similarity loss between the current word and k most probable words
                            loss=cosine_loss(weights[curr_word], weights[best_word]).numpy() 
                            # if the cosine similarity loss is lower than the threshold, current word and most probable word have similar meaning, and should not be a anomalous word 
                            if loss<cosine_threshold:
                                append_number=word_probability
                                break
                            # otherwise keep iterating through next most probable word in k   
                            else:                            
                                continue
                        entire_sentence_word_p.append(append_number)
            # if no next-word probabilty is found in GPT-2, the word is seen as anomalous                                  
            elif word_probability==None:
                entire_sentence_word_p.append(-23)
            # if the next-word probability of the current word is higher than a threshold, append the original next-word probability
            else:
                entire_sentence_word_p.append(word_probability)
        # get the mean of next-word probabilities in a sentence           
        entire_sentence_word_p=np.array(entire_sentence_word_p)   
        entire_sentence_word_probability_mean=np.mean(entire_sentence_word_p)
        # process the case when sentence list contains nothing  
        if (np.isnan(entire_sentence_word_probability_mean)).any()==True:
            entire_sentence_word_probability_mean=-10
        sentence_probability.append(entire_sentence_word_probability_mean)
    # Use a classifer to process all the evaluation sentneces: if sentence probability is negative, it is an anomalous sentence, otherwirse a natural sentence     
    result=get_result(sentence_probability, 0)  
    return result

# Position probability conditioned method (3.4)
def detect_anamoly_sentence_position(dataset, best_probability): 
    result=[] 
    word_probability=[] 
    #import GPT-2
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    #import tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    data_converted=[]    
    sentence_probability=[]
    # import softmax layer
    m = nn.Softmax(dim=1)
    # iterate through all the sentences in the evaluation set 
    for index, eval_sentence in enumerate(dataset): 
        # tokenize words in sentences to evaluate
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        # get next-word probabilities of words in a evaluation sentence
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[]
        # generate the probabilties of words in a evaluation sentence using the relevant next-word probabilities of GPT-2
        for i in range(1, len(tokens_tensor)-1): 
            # get the current token and the previous token to examine
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            softmax_output = m(outputs[1][:,i-1,:])
            # get the next-word probability of the current word
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy()  
            # generate position probability of the current word
            entire_position_probability=m(outputs[1][:, :,curr_word])
            position_probability=entire_position_probability[0][i-1]
            position_probability=position_probability.detach().numpy()
            word_position_probability=position_probability/1
            # take log to avoid an overflow of digits
            word_probability=np.log(word_probability)
            # if probabiltiy is not zero, substract position probability (condition probabilty on position probability)
            if word_position_probability != 0.0:
              # take log to avoid an overflow of digits
              word_position_probability=np.log(word_position_probability)
            word_probability_conditioned_position=word_probability-word_position_probability
            # if the next-word probability of the current word is higher than a threshold, set the word probability to a large positive number
            if word_probability_conditioned_position > best_probability:   
              word_probability_conditioned_position=best_probability*len(tokens_tensor)
            # if the next-word probability of the current word is lower than a threshold, append the original next-word probability  
            entire_sentence_word_p.append(word_probability_conditioned_position)       
        # process the case when word sentence list contains nothing  
        if len(entire_sentence_word_p)==0 :
            sentence_probability.append(0.0)
        else:
            entire_sentence_word_probability_mean=np.mean(entire_sentence_word_p) 
            # get the mean of next-word probabilities in a sentence 
            sentence_probability.append(entire_sentence_word_probability_mean)     
    # Use a classifer to process all the evaluation sentneces: if sentence probability is larger than a threshold, it is an anomalous sentence, otherwirse a natural sentence              
    for sentence in sentence_probability:
        if sentence>=best_probability:
            result.append(1)
        else:
            result.append(0)        
    return result

# Position probability conditioned method (3.4)
def detect_anamoly_sentence_position_threshold(dataset, y_test): 
    result=[] 
    word_probability=[]
    #import GPT-2 
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    #import tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    data_converted=[]    
    sentence_probability=[]
    # import softmax layer
    m = nn.Softmax(dim=1)
    # iterate through all the sentences in the evaluation set 
    for eval_sentence in dataset: 
         # tokenize words in sentences to evaluate
        inputs = tokenizer(eval_sentence, return_tensors="pt")
        # get next-word probabilities of words in a evaluation sentence
        tokens_tensor=tokenizer.encode(eval_sentence, add_special_tokens=False, max_length=1020, truncation=True)
        outputs = model(**inputs, labels=inputs["input_ids"])    
        entire_sentence_word_p=[]
        # generate the probabilties of words in a evaluation sentence using the relevant next-word probabilities of GPT-2
        for i in range(1, len(tokens_tensor)-1): 
            # get the current token and the previous token to examine
            pre_word=tokens_tensor[i-1] 
            curr_word=tokens_tensor[i] 
            # get the next-word probability of the current word
            softmax_output = m(outputs[1][:,i-1,:])
            word_probability=softmax_output[0][curr_word]
            word_probability=word_probability.detach().numpy() 
            # generate position probability of the current word 
            entire_position_probability=m(outputs[1][:, :,curr_word])
            position_probability=entire_position_probability[0][i-1]
            position_probability=position_probability.detach().numpy()
            word_position_probability=position_probability/1
            # take log to avoid an overflow of digits
            word_probability=np.log(word_probability)
            # if probabiltiy is not zero, substract position probability (condition probabilty on position probability)
            if word_position_probability != 0.0:
              # take log to avoid an overflow of digits
              word_position_probability=np.log(word_position_probability)
            word_probability_conditioned_position=word_probability-word_position_probability
            # append the conditioned probability to a sentence list
            entire_sentence_word_p.append(word_probability_conditioned_position)
        # process the case when word sentence list contains nothing     
        if len(entire_sentence_word_p)==0 :
            sentence_probability.append(0.0)
        else:
        # get the mean of next-word probabilities in a sentence   
            entire_sentence_word_probability_mean=np.mean(entire_sentence_word_p) 
            sentence_probability.append(entire_sentence_word_probability_mean) 
    # Use Yoden's Index to find an optimal threshold             
    threshold=get_threshold(y_test, sentence_probability)
    # use the optimal threshold to detect the anomalous sentence
    for sentence in sentence_probability:
        if sentence<threshold:
            result.append(1)
        else:
            result.append(0)        
    return result